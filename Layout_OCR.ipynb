{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.ai.vision as sdk\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "import azure.ai.vision as sdk\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "RSEED = 42\n",
    "\n",
    "\n",
    "service_options = sdk.VisionServiceOptions(\n",
    "    os.environ[\"VISION_ENDPOINT\"], os.environ[\"VISION_KEY\"])\n",
    "\n",
    "\n",
    "def create_prediction_from_file(filename):\n",
    "    \"\"\"     \n",
    "    Performs an image analysis using the vision analysis service.\n",
    "\n",
    "    :param filename: str\n",
    "        The path of the image file to be analyzed.\n",
    "\n",
    "    :return: dict\n",
    "        A dictionary containing the result of the image analysis in JSON format.\n",
    "\n",
    "    Example:\n",
    "    >>> file_path = \"path/to/image.jpg\"\n",
    "    >>> result_json = create_prediction_from_file(file_path)\n",
    "    >>> print(result_json)\n",
    "    \"\"\"\n",
    "    vision_source = sdk.VisionSource(filename=filename)\n",
    "    analysis_options = sdk.ImageAnalysisOptions()\n",
    "\n",
    "    analysis_options.model_name = \"historicalink13\"\n",
    "    analysis_options.language = \"es\"\n",
    "    analysis_options.gender_neutral_caption = True\n",
    "    image_analyzer = sdk.ImageAnalyzer(\n",
    "        service_options, vision_source, analysis_options)\n",
    "\n",
    "    result = image_analyzer.analyze()\n",
    "\n",
    "    if result.reason == sdk.ImageAnalysisResultReason.ANALYZED:\n",
    "\n",
    "        result_details = sdk.ImageAnalysisResultDetails.from_result(result)\n",
    "        return result_details.json_result\n",
    "\n",
    "    else:\n",
    "        error_details = sdk.ImageAnalysisErrorDetails.from_result(result)\n",
    "        print(\" Analysis failed.\")\n",
    "        print(\"   Error reason: {}\".format(error_details.reason))\n",
    "        print(\"   Error code: {}\".format(error_details.error_code))\n",
    "        print(\"   Error message: {}\".format(error_details.message))\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kmeans_best(X, min_cols=2, max_cols=4):\n",
    "    \"\"\"\n",
    "    Performs the K-Means algorithm with different numbers of clusters and returns the labels\n",
    "    of the best model according to the silhouette coefficient.\n",
    "\n",
    "    :param X: array-like or pd.DataFrame\n",
    "        The input data for the K-Means algorithm.\n",
    "\n",
    "    :param min_cols: int, optional, default: 2\n",
    "        The minimum number of clusters to try.\n",
    "\n",
    "    :param max_cols: int, optional, default: 4\n",
    "        The maximum number of clusters to try.\n",
    "\n",
    "    :return: array\n",
    "        The labels of the best K-Means model according to the silhouette coefficient.\n",
    "\n",
    "    Example:\n",
    "    >>> from sklearn.datasets import make_blobs\n",
    "    >>> X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "    >>> best_labels = kmeans_best(X, min_cols=2, max_cols=5)\n",
    "    >>> print(best_labels)\n",
    "    \"\"\"\n",
    "    sil_score_max = -1\n",
    "\n",
    "    for n_clusters in range(min_cols, max_cols+1):\n",
    "        model = KMeans(n_clusters=n_clusters, init='k-means++',\n",
    "                       max_iter=100, n_init=1)\n",
    "        labels = model.fit_predict(X)\n",
    "        sil_score = silhouette_score(X, labels)\n",
    "        if sil_score > sil_score_max:\n",
    "            sil_score_max = sil_score\n",
    "            best_labels = labels\n",
    "    return best_labels\n",
    "\n",
    "\n",
    "flog = open(\"logfile.log\", \"a\")\n",
    "\n",
    "\n",
    "def ocr(filename):\n",
    "    \"\"\"\n",
    "    Performs OCR analysis on an image using Azure Computer Vision.\n",
    "\n",
    "    :param filename: str\n",
    "        The path of the image file to be analyzed.\n",
    "\n",
    "    :return: list\n",
    "        A list of dictionaries containing information about the detected text.\n",
    "        Each dictionary has the keys 'id', 'text', 'bounding_box', and 'center'.\n",
    "\n",
    "    Example:\n",
    "    >>> from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "    >>> from msrest.authentication import CognitiveServicesCredentials\n",
    "    >>> service_options = ComputerVisionClient(endpoint=\"YOUR_ENDPOINT\", credentials=CognitiveServicesCredentials(\"YOUR_SUBSCRIPTION_KEY\"))\n",
    "    >>> flog = open(\"error_log.txt\", \"w\")\n",
    "    >>> filename = \"path/to/image.jpg\"\n",
    "    >>> result_text = ocr(filename, service_options, flog)\n",
    "    >>> print(result_text)\n",
    "    \"\"\"\n",
    "\n",
    "    vision_source = sdk.VisionSource(\n",
    "        filename=filename)\n",
    "    analysis_options = sdk.ImageAnalysisOptions()\n",
    "    analysis_options.features = (\n",
    "        sdk.ImageAnalysisFeature.TEXT\n",
    "    )\n",
    "    analysis_options.language = \"es\"\n",
    "    analysis_options.gender_neutral_caption = True\n",
    "    image_analyzer = sdk.ImageAnalyzer(\n",
    "        service_options, vision_source, analysis_options)\n",
    "    result = image_analyzer.analyze()\n",
    "    texts = []\n",
    "    polygons = []\n",
    "    if result.reason == sdk.ImageAnalysisResultReason.ANALYZED:\n",
    "        if result.text is not None:\n",
    "            for line in result.text.lines:\n",
    "                texts.append(line.content)\n",
    "                polygons.append(line.bounding_polygon)\n",
    "    else:\n",
    "        error_details = sdk.ImageAnalysisErrorDetails.from_result(result)\n",
    "        print(\" Analysis failed.\")\n",
    "        print(\"   Error reason: {}\".format(error_details.reason))\n",
    "        print(\"   Error code: {}\".format(error_details.error_code))\n",
    "        print(\"   Error message: {}\".format(error_details.message))\n",
    "        print(\n",
    "            f\"ERROR EN IMAGEN {filename} {error_details.reason} {error_details.message} {error_details.error_code}\")\n",
    "        flog.write(\n",
    "            f\"ERROR EN IMAGEN {filename} {error_details.reason} {error_details.message} {error_details.error_code}\\n\")\n",
    "    textos = []\n",
    "    if len(polygons) > 5:\n",
    "        data = np.array(polygons)\n",
    "        data_text = np.array(texts)\n",
    "        first_col = data[:, 0].reshape((len(data), 1))\n",
    "        indices_nn = kmeans_best(first_col, 2, 5)\n",
    "        labels = np.unique(indices_nn)\n",
    "        for i, label in enumerate(labels):\n",
    "            idx = indices_nn == label\n",
    "            texto = \"\"\n",
    "            textos_columna = data_text[idx]\n",
    "            coords_columna = data[idx]\n",
    "            for t in textos_columna:\n",
    "                t = t.strip()\n",
    "                if \"-\" in t[-1]:\n",
    "                    texto += t[:-1] + \"\"\n",
    "                else:\n",
    "                    texto += t + \" \"\n",
    "\n",
    "            minimos = np.min(coords_columna, axis=0)\n",
    "            maximos = np.max(coords_columna, axis=0)\n",
    "            coords = np.array([minimos, maximos])\n",
    "            minx = np.min(minimos[::2])\n",
    "            miny = np.min(minimos[1::2])\n",
    "\n",
    "            maxx = np.max(maximos[::2])\n",
    "            maxy = np.max(maximos[1::2])\n",
    "            textos.append(\n",
    "                {\"id\": i, \"text\": texto, \"bounding_box\": [minx, miny, maxx, maxy], \"center\": [(minx + maxx)/2, (miny + maxy)/2]})\n",
    "    return textos\n",
    "\n",
    "\n",
    "def dist(p0, p1):\n",
    "    \"\"\"\n",
    "    Calculates the Euclidean distance between two points in space.\n",
    "\n",
    "    :param p0: array-like\n",
    "        Coordinates of the first point.\n",
    "\n",
    "    :param p1: array-like\n",
    "        Coordinates of the second point.\n",
    "\n",
    "    :return: float\n",
    "        The Euclidean distance between the two points.\n",
    "\n",
    "    Example:\n",
    "    >>> point1 = [1, 2, 3]\n",
    "    >>> point2 = [4, 5, 6]\n",
    "    >>> distance = dist(point1, point2)\n",
    "    >>> print(distance)\n",
    "    5.196152422706632\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(np.array(p0)-np.array(p1))\n",
    "\n",
    "\n",
    "def process_image_from_predictions(img_path, predictionsJSON, dest_folder, th=0.45, metadata=None):\n",
    "    \"\"\"\n",
    "    Crops regions of interest (ROI) from an image according to the predictions of a custom model.\n",
    "    The regions of interest include objects with confidence greater than or equal to a given threshold.\n",
    "\n",
    "    :param img_path: str\n",
    "        Path of the input image.\n",
    "\n",
    "    :param predictionsJSON: dict\n",
    "        Prediction results from the model in JSON format.\n",
    "\n",
    "    :param dest_folder: str\n",
    "        Destination folder to save the cropped images and the JSON results.\n",
    "\n",
    "    :param th: float, optional, default: 0.45\n",
    "        Confidence threshold to include an object in the regions of interest.\n",
    "\n",
    "    :param metadata: dict, optional, default: None\n",
    "        Additional metadata that can be included in the JSON result.\n",
    "\n",
    "    :return: dict\n",
    "        A dictionary containing information about the cropped images, detected texts, and metadata.\n",
    "        A JSON file with this dictionary is saved in the destination folder.\n",
    "\n",
    "    Example:\n",
    "    >>> img_path = \"path/to/image.jpg\"\n",
    "    >>> predictionsJSON = create_prediction_from_file(file_path)\n",
    "    >>> dest_folder = \"destination/path\"\n",
    "    >>> metadata = {\"author\": \"John Doe\"}\n",
    "    >>> result_dict = process_image_from_predictions(img_path, predictionsJSON, dest_folder, th=0.5, metadata=metadata)\n",
    "    >>> print(result_dict)\n",
    "    \"\"\"\n",
    "    im = Image.open(img_path)\n",
    "    data = predictionsJSON[\"customModelResult\"][\"objectsResult\"][\"values\"]\n",
    "    original_size = int(predictionsJSON[\"metadata\"][\"width\"]), int(\n",
    "        predictionsJSON[\"metadata\"][\"height\"])\n",
    "    only_text = Image.new(mode=\"RGB\", size=original_size, color=\"white\")\n",
    "\n",
    "    basename_noextension = os.path.basename(img_path).split(\".\")[0]\n",
    "    hay_texto = False\n",
    "    images_bb = []\n",
    "    imdraw = ImageDraw.Draw(only_text)\n",
    "    images = []\n",
    "    for i, segment in enumerate(data):\n",
    "        confidence = segment[\"tags\"][0][\"confidence\"]\n",
    "        if confidence >= th:\n",
    "            x = segment[\"boundingBox\"][\"x\"]\n",
    "            y = segment[\"boundingBox\"][\"y\"]\n",
    "            w = segment[\"boundingBox\"][\"w\"]\n",
    "            h = segment[\"boundingBox\"][\"h\"]\n",
    "            left = x\n",
    "            top = y\n",
    "            right = x + w\n",
    "            bottom = y + h\n",
    "            imi = im.crop((left, top, right, bottom))\n",
    "\n",
    "            if \"image\" in segment[\"tags\"][0][\"name\"].lower():\n",
    "                imi.save(\n",
    "                    f\"{dest_folder}/images/{basename_noextension}_{i}.jpg\")\n",
    "                images_bb.append([left, top, right, bottom])\n",
    "                image = {}\n",
    "                image[\"center\"] = [(left+right)/2, (top+bottom)/2]\n",
    "                image[\"bounding_box\"] = [left, top, right, bottom]\n",
    "                image[\"context\"] = []\n",
    "                image[\"filename\"] = f\"{dest_folder}/images/{basename_noextension}_{i}.jpg\"\n",
    "                images.append(image)\n",
    "            else:\n",
    "                hay_texto = True\n",
    "                only_text.paste(imi, (int(left), int(top)))\n",
    "    list_textos = []\n",
    "    if hay_texto:\n",
    "        for left, top, right, bottom in images_bb:\n",
    "            imdraw.rectangle(\n",
    "                [left, top, right, bottom], fill=None, outline=\"red\")\n",
    "        only_text.save(\n",
    "            f\"{dest_folder}/text/{basename_noextension}.jpg\")\n",
    "        list_textos = ocr(\n",
    "            f\"{dest_folder}/text/{basename_noextension}.jpg\")\n",
    "        for te in list_textos:\n",
    "            left, top, right, bottom = te[\"bounding_box\"]\n",
    "            imdraw.rectangle(\n",
    "                [left, top, right, bottom], fill=None, outline=\"blue\")\n",
    "            distancias = []\n",
    "            for im in images:\n",
    "                dista = dist(te[\"center\"], im[\"center\"])\n",
    "                distancias.append(dista)\n",
    "            if distancias:\n",
    "                id_img = np.argmin(distancias)\n",
    "                images[id_img][\"context\"].append(te[\"id\"])\n",
    "\n",
    "        only_text.save(\n",
    "            f\"{dest_folder}/text/{basename_noextension}.jpg\")\n",
    "\n",
    "    metadata = metadata or {}\n",
    "    dictio = {\"metadata\": metadata, \"contexts\": list_textos, \"images\": images}\n",
    "    open(f\"{dest_folder}/{basename_noextension}.json\",\n",
    "         \"w\", encoding=\"utf-8\").write(json.dumps(dictio, ensure_ascii=False))\n",
    "    return dictio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"Inventario caricaturas prensa SXIX.xlsx\")\n",
    "df = df[[\"IDPublicación\", \"Año\", \"Ciudad\"]]\n",
    "df.dropna(inplace=True)\n",
    "df[\"ID\"] = df[\"IDPublicación\"].str.replace(\" - \", \"\")\n",
    "df.set_index(\"ID\", inplace=True)\n",
    "del df[\"IDPublicación\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processes newspaper data stored in an input folder, makes predictions, and saves results in an output folder.\n",
    "\n",
    ":param input_folder: str\n",
    "    Path of the folder containing the newspaper data.\n",
    "\n",
    ":param output_folder: str\n",
    "    Path of the output folder to save the results.\n",
    "\"\"\"\n",
    "\n",
    "OUTPUT = \"OUTPUT\"\n",
    "\n",
    "try:\n",
    "\n",
    "    os.mkdir(f\"{OUTPUT}\")\n",
    "\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "INPUT_FOLDER = \"data\"\n",
    "\n",
    "newspapers = os.listdir(INPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "OUTPUT_FOLDER_ISSUE = r\"OUTPUT\\PD168_El oso\\1_results\"\n",
    "\n",
    "\n",
    "def asign_lost_contexts(output_folder_issue):\n",
    "    pages = os.listdir(output_folder_issue)\n",
    "    pages.remove(\"images\")\n",
    "    pages.remove(\"text\")\n",
    "    pages_numbers = [int(i.split(\"_\")[-1].split(\".\")[0]) for i in pages]\n",
    "    idxs = np.argsort(pages_numbers)\n",
    "    pages = np.array(pages)[idxs]\n",
    "    context_voids = []\n",
    "    full_issue = {}\n",
    "    for page in pages:\n",
    "        filename = f\"{output_folder_issue}/{page}\"\n",
    "        page_info = json.load(open(filename, encoding=\"utf-8\"))\n",
    "        full_issue[page] = page_info\n",
    "        images = page_info[\"images\"]\n",
    "        page_number = page_info[\"metadata\"][\"page\"]\n",
    "        for image in images:\n",
    "            context_voids.append(page)\n",
    "            original_context = image[\"context\"]\n",
    "            new_context = []\n",
    "            for ctx in original_context:\n",
    "                new_context.append(f\"{page_number}_{ctx}\")\n",
    "            image[\"context\"] = new_context\n",
    "    if context_voids:\n",
    "        page = context_voids[0]\n",
    "        page_number_parsed = int(context_voids[0].split(\"_\")[-1].split(\".\")[0])\n",
    "        n_page = context_voids[-1]\n",
    "        page_number_parsed_r = int(\n",
    "            context_voids[-1].split(\"_\")[-1].split(\".\")[0])\n",
    "        paginas_inicio_sin_foto = pages[:page_number_parsed]\n",
    "        paginas_final_sin_foto = pages[page_number_parsed_r+1:]\n",
    "        first_page = full_issue[page]\n",
    "        last_page = full_issue[n_page]\n",
    "        for paaaa in paginas_inicio_sin_foto:\n",
    "            page_number = full_issue[paaaa][\"metadata\"][\"page\"]\n",
    "            contexts = full_issue[paaaa][\"contexts\"]\n",
    "            first_page[\"images\"][0][\"context\"].extend(\n",
    "                                [f\"{page_number}_{i['id']}\" for i in contexts])\n",
    "        for paaaa in paginas_final_sin_foto:\n",
    "            page_number = full_issue[paaaa][\"metadata\"][\"page\"]\n",
    "            contexts = full_issue[paaaa][\"contexts\"]\n",
    "            last_page[\"images\"][-1][\"context\"].extend(\n",
    "                [f\"{page_number}_{i['id']}\" for i in contexts])\n",
    "\n",
    "        for i in range(len(context_voids)-1):\n",
    "            page = context_voids[i]\n",
    "            page_number_parsed = int(\n",
    "                context_voids[i].split(\"_\")[-1].split(\".\")[0])\n",
    "            n_page = context_voids[i+1]\n",
    "            page_number_parsed_r = int(\n",
    "                context_voids[i+1].split(\"_\")[-1].split(\".\")[0])\n",
    "            left_page = full_issue[page]\n",
    "            right_page = full_issue[n_page]\n",
    "\n",
    "            paginas_void = pages[page_number_parsed+1:page_number_parsed_r]\n",
    "            n = len(paginas_void)\n",
    "            if n != 0:\n",
    "                if n % 2 == 0:\n",
    "                    mitad1 = paginas_void[:n//2]\n",
    "                    mitad2 = paginas_void[n//2:]\n",
    "                    for pag in mitad1:\n",
    "                        page_info = full_issue[pag]\n",
    "                        page_number = page_info[\"metadata\"][\"page\"]\n",
    "                        contexts = page_info[\"contexts\"]\n",
    "                        left_page[\"images\"][-1][\"context\"].extend(\n",
    "                            [f\"{page_number}_{i['id']}\" for i in contexts])\n",
    "                    for pag in mitad2:\n",
    "                        page_info = full_issue[pag]\n",
    "                        page_number = page_info[\"metadata\"][\"page\"]\n",
    "                        contexts = page_info[\"contexts\"]\n",
    "                        right_page[\"images\"][0][\"context\"].extend(\n",
    "                            [f\"{page_number}_{i['id']}\" for i in contexts])\n",
    "                else:\n",
    "                    mitad1 = paginas_void[:n//2]\n",
    "                    mitad2 = paginas_void[n//2:][1:]\n",
    "                    medio = paginas_void[n//2]\n",
    "                    for pag in mitad1:\n",
    "                        page_info = full_issue[pag]\n",
    "                        page_number = page_info[\"metadata\"][\"page\"]\n",
    "                        contexts = page_info[\"contexts\"]\n",
    "                        left_page[\"images\"][-1][\"context\"].extend(\n",
    "                            [f\"{page_number}_{i['id']}\" for i in contexts])\n",
    "                    for pag in mitad2:\n",
    "                        page_info = full_issue[pag]\n",
    "                        page_number = page_info[\"metadata\"][\"page\"]\n",
    "                        contexts = page_info[\"contexts\"]\n",
    "                        right_page[\"images\"][0][\"context\"].extend(\n",
    "                            [f\"{page_number}_{i['id']}\" for i in contexts])\n",
    "                    page_info = full_issue[medio]\n",
    "                    contexts = page_info[\"contexts\"]\n",
    "                    m = len(contexts)\n",
    "                    page_number_medio = page_info[\"metadata\"][\"page\"]\n",
    "                    left_page[\"images\"][-1][\"context\"].extend(\n",
    "                        [f\"{page_number_medio}_{i['id']}\" for i in contexts[m//2:]])\n",
    "                    right_page[\"images\"][0][\"context\"].extend(\n",
    "                        [f\"{page_number_medio}_{i['id']}\" for i in contexts[:m//2]])\n",
    "        for name, dic in full_issue.items():\n",
    "            filename = f\"{output_folder_issue}/{name}\"\n",
    "            json.dump(dic, open(filename, \"w\", encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "# asign_lost_contexts(OUTPUT_FOLDER_ISSUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisados = os.listdir(OUTPUT)\n",
    "no_revisados = set(newspapers)-set(revisados)\n",
    "np.savetxt(\"faltantes.txt\", list(no_revisados), encoding=\"utf-8\", fmt=\"%s\")\n",
    "for NESPAPER_NAME in tqdm(no_revisados):\n",
    "    newspaper_issues = set(os.listdir(f\"{INPUT_FOLDER}/{NESPAPER_NAME}\"))\n",
    "    newspaper_output_folder = f\"{OUTPUT}/{NESPAPER_NAME}\"\n",
    "    try:\n",
    "        os.makedirs(f\"{newspaper_output_folder}\", exist_ok=True)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    issues_revised = set([i[:-8] for i in os.listdir(newspaper_output_folder)])\n",
    "    issues_not_revised = newspaper_issues - issues_revised\n",
    "    for issue in issues_not_revised:\n",
    "        OUTPUT_FOLDER = f\"{OUTPUT}/{NESPAPER_NAME}/{issue}_results\"\n",
    "        print(OUTPUT_FOLDER)\n",
    "        try:\n",
    "            os.makedirs(f\"{OUTPUT_FOLDER}\", exist_ok=True)\n",
    "        except Exception as e:\n",
    "\n",
    "            pass\n",
    "        # OUTPUT_FOLDER es OUTPUT_FOLDER_ISSUE\n",
    "        try:\n",
    "            os.makedirs(f\"{OUTPUT_FOLDER}/images\", exist_ok=True)\n",
    "            os.makedirs(f\"{OUTPUT_FOLDER}/text\", exist_ok=True)\n",
    "        except Exception as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        NPFolder = f\"{INPUT_FOLDER}/{NESPAPER_NAME}/{issue}\"\n",
    "        files = [f\"{NPFolder}/{i}\" for i in os.listdir(NPFolder)]\n",
    "        for filename in tqdm(files):\n",
    "            nombre_periodico, archivo, pagina = filename.split(\"/\")[1:]\n",
    "            if \".\" in pagina:\n",
    "                pagina = pagina.split(\".\")[0]\n",
    "            id_periodico = nombre_periodico.split(\"_\")[0]\n",
    "            metadata = {\"id\": id_periodico,\n",
    "                        \"newspaper\": nombre_periodico,\n",
    "                        \"year\": df[\"Año\"].get(id_periodico, \"No registra\"),\n",
    "                        \"city\": df[\"Ciudad\"].get(id_periodico, \"No registra\"),\n",
    "                        \"file\": archivo,\n",
    "                        \"page\": pagina}\n",
    "            results = create_prediction_from_file(filename)\n",
    "\n",
    "            if results:\n",
    "                results = json.loads(results)\n",
    "                try:\n",
    "                    res = process_image_from_predictions(\n",
    "                        filename, results, OUTPUT_FOLDER, metadata=metadata)\n",
    "                except Exception as e:\n",
    "                    print(\"Problema con imagen\", filename)\n",
    "        # nueva funcion\n",
    "        asign_lost_contexts(output_folder_issue=OUTPUT_FOLDER)\n",
    "\n",
    "flog.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
